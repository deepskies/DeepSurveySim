PPO_HYPERPARAMS: {
    gamma: 0.9,
    lr: 0.001,
    lr_schedule: [ [ 0, 0.0003 ], [ 10000, 0.0001 ] ],
    use_critic: True,
    use_gae: True,
    #lambda: 0.95,
    kl_coeff: 0.2,
    sgd_minibatch_size: 128,
    num_sgd_iter: 10,
    shuffle_sequences: True,
    vf_loss_coeff: 0.5,
    entropy_coeff: 0.01,
    entropy_coeff_schedule: [ [ 0, 0.01 ], [ 10000, 0.001 ] ],
    clip_param: 0.2,
    vf_clip_param: 10.0,
    grad_clip: 0.5,
    kl_target: 0.01
}

SAC_HYPERPARAMS: {
    twin_q: True,
    q_model_config: {},
    policy_model_config: {},
    tau: 0.005,
    initial_alpha: 0.1,
    target_entropy: "auto",
    n_step: 1,
    store_buffer_in_checkpoints: False,
    replay_buffer_config: {
        "_enable_replay_buffer_api": True,
        "type": "SimpleReplayBuffer",
        "capacity": 100000,
        "max_size": 100000
    },
    training_intensity: None,
    clip_actions: False,
    grad_clip: 0.05,
    optimization_config: {
        "actor_learning_rate": 0.0003,
        "critic_learning_rate": 0.0003,
        "entropy_learning_rate": 0.0003
    },
    target_network_update_freq: 1,
    _deterministic_loss: False,
    _use_beta_distribution: False,
}

DDPG_HYPERPARAMS: {
    twin_q: True,
    policy_delay: 1,
    smooth_target_policy: True,
    target_noise: 0.2,
    target_noise_clip: 0.5,
    use_state_preprocessor: False,
    actor_hiddens: [ 256, 256 ],
    actor_hidden_activation: "relu",
    critic_hiddens: [ 256, 256 ],
    critic_hidden_activation: "relu",
    n_step: 1,
    critic_lr: 0.001,
    actor_lr: 0.001,
    tau: 0.005,
    use_huber: False,
    huber_threshold: 1.0,
    l2_reg: 0.0,
    training_intensity: None
}




